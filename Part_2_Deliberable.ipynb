{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = '14kkcuU6wd9UMvjaDrg3PNI-e_voCi8HL'\n",
    "nwpu_images_path = 'NWPU_images.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download NWPU-RESISC45 satellite imagery (Google Drive)\n",
    "Warning: will download 405MB. Will only use images of lakes, and then only those that already have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# From Part_1_GettingStarted.ipynb\n",
    "# Which is in turn from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "def get_file(id, destination):\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "        \n",
    "        with open(destination, 'wb') as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk: # Filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "    \n",
    "    URL = 'https://docs.google.com/uc?export=download'\n",
    "    \n",
    "    session = requests.Session()\n",
    "    \n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "    \n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "    \n",
    "    save_response_content(response, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip(path):\n",
    "    with zipfile.ZipFile(f, 'r') as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "get_file(file_id, nwpu_images_path)\n",
    "unzip(nwpu_images_path)\n",
    "\n",
    "# Rename folder for clarity\n",
    "try:\n",
    "    os.rename('images', 'nwpu_images')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Remove everything that isn't a picture of a lake\n",
    "non_lake_subdirs = [s for s in [x[0] for x in os.walk('nwpu_images')][1:] if 'lake' not in s]\n",
    "for subdir in non_lake_subdirs:\n",
    "    shutil.rmtree(subdir, ignore_errors=True)\n",
    "\n",
    "# Rename lake subdir to 'data'\n",
    "os.rename('nwpu_images' + os.sep + 'lake', 'nwpu_images' + os.sep + 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create labels\n",
    "This is a manual process, and must be done through [makesense.io](https://makesense.io). We'll use labels that were already created/in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename, segment into folders (training_images, validation_images, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_images_from_polys(polys, folder_name):\n",
    "    for filename in polys:\n",
    "        shutil.move(\n",
    "            'nwpu_images' + os.sep + 'data' + os.sep + filename,\n",
    "            'nwpu_images' + os.sep + 'data' + os.sep + folder_name + os.sep + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_polys = json.load(open('nwpu_labels' + os.sep + 'nwpu_lakes_30samples.json'))\n",
    "validation_polys = json.load(open('nwpu_labels' + os.sep + 'nwpu_lakes_20samplesA.json'))\n",
    "test_polys = json.load(open('nwpu_labels' + os.sep + 'nwpu_lakes_20samplesB.json'))\n",
    "\n",
    "try:\n",
    "    os.mkdir('nwpu_images' + os.sep + 'data' + os.sep + 'training')\n",
    "    os.mkdir('nwpu_images' + os.sep + 'data' + os.sep + 'validation')\n",
    "    os.mkdir('nwpu_images' + os.sep + 'data' + os.sep + 'test')\n",
    "\n",
    "    move_images_from_polys(training_polys, 'training')\n",
    "    move_images_from_polys(validation_polys, 'validation')\n",
    "    move_images_from_polys(test_polys, 'test')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('nwpu_images' + os.sep + 'data'):\n",
    "    if (filename.endswith('.jpg')):\n",
    "        os.remove('nwpu_images' + os.sep + 'data' + os.sep + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
